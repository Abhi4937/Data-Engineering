DATA Lake:
	It is a centralized data repository that allows you to store structured and unstructed data at any scale.
	Unlike the traditional databases or data warehouse, which stores data in predefined schemas, data lake can store data in raw form.
	
Characteristics of data lake:
1. storage of divers data type:
	1. Sturctured data: Data that has a defined schema,such as relational data from databases 	(e.g., CSV, tables)
	2. Semi-Sturctured data: Data that have some organizational property but does not have a 	fixed schema (e.g., JSON, XML)
	3. Unstructured data: Raw data that does not have any predifined schema (e.g., images, 	videos, and sensor data)
2. Schema-on-Read:
	1. unlike traditional databases(which uses schema-on-write approach), data lakes uses 	schema-on-read method. This means data is stored in raw format and schema is applied 	when data is queried/read
3. Scalability: 
	1. Data lake can store petabytes of data due to scalable nature of the storage 	architecture. cloud platforms like(Amazon S3,  Azure data lake storage, Google cloud 	storage) are common choices of hosting data lakes
4. low cost storage:
	1. Since data lake store data in raw format, the cost per Gigabyte is typically lower 	compared to databases and warehouses that involves complext storage management.
5. Analytics and Machine learning:
	1. Data lake allow data scientists and analysts to run big data analytics, machine learning, 	algorithms, and BI tools directly on raw data.
	
Data Lake Architecture:
	1. Ingestion Layer: This is where data is ingested form various sources (IoT devices, 	databases, APIs) in real time or in batches.
	2. Storage Layer: Raw data is stored in data lake, and this layer supports  different types of 	storage format (e.g., Parquet, ORC, Avro)
	3. Processing Layer: Data Processing engines (e.g., Apache Spark, AWS Glue) are used to 	perform ETL operations on the data.
	4. Governance & Security: Tools for managing access control, data quality, and security to 	ensure both data is proctected and usable.
	
Benefits of Data Lakes:
	1. Flexible: Handle a wide range of data types without the need for a strict schema.
	2. Cost-efficient: Ideal for companies that need to store massive amounts of data without 	high upfront costs.
	3. Advance analytics: Facilitates advanced data analytics, machine learning, and AI by 	providing easy access to large volumes of raw data.
	
Examples: 
	1. Log and Event Data: Storing application logs and IoT sensor data for analysis and 	anomaly detection.
	2. Data for Machine Learning: Data lakes can hold training data for machine learning 	models, and the same data can be reused for other experiments.
	3. Media Storage: Storing images, video, and audio files for content-heavy industries (e.g., 	streaming platforms).

Data lake examples:
1. Cloud-Based Data Lake Solutions
Amazon S3 as Storage:
	1. S3 is primarily a storage service, meaning its main role is to store data in its raw 	format 	(structured, semi-structured, and unstructured).
	2. By itself, S3 does not perform data processing, governance, or querying—it only stores 	data.

Microsoft Azure Data Lake Storage (ADLS)
	Description: A scalable data lake service on Azure that integrates with other Azure services 	for data analytics and processing.
	Use Case: BMW uses Azure Data Lake to process and analyze data from vehicles, 	leveraging the data for operational insights and customer service enhancements.

Google Cloud Storage (GCS)
	Description: A unified object storage service that acts as a data lake for various data types.
	Use Case: Spotify employs Google Cloud Storage to store user data and analyze listening 	habits, facilitating personalized playlists and recommendations.

Snowflake
	Description: A cloud-based data platform that functions as both a data lake and a data 	warehouse, allowing for scalable data storage and analytics.
	Use Case: Epic Games uses Snowflake to manage data for its gaming services, enabling 	real-time analytics and player behavior insights.

Databricks Lakehouse Platform
	Description: Combines data lake and data warehouse functionalities, built on Apache 	Spark.
	Use Case: Shell uses Databricks for real-time analytics on sensor data from oil rigs, 	improving operational efficiency and safety.

On-Premises and Hybrid Data Lake Solutions:

Cloudera Data Platform (CDP)
	Description: A hybrid cloud data platform that provides data lake capabilities for both on-	premises and cloud environments.
	Use Case: Deutsche Telekom uses Cloudera to manage a data lake that helps analyze 	customer data to improve service delivery.

Apache Hadoop HDFS
	Description: A distributed file system that allows for the storage of large data sets across 	clusters of machines.
	Use Case: Yahoo has built its data lake using Hadoop to manage user-generated data for 	advertising and content personalization.

IBM Cloud Object Storage
	Description: A scalable storage service that can be used to build a data lake, integrating 	with IBM's analytics and AI services.
	Use Case: Lufthansa utilizes IBM Cloud Object Storage to analyze flight data, customer 	interactions, and maintenance logs.

3. Enterprise Data Lake Use Cases:
Uber
Platform: Built using technologies like Hadoop and Apache Hive.
Use Case: Uber stores vast amounts of trip data, user interactions, and GPS logs in its data lake for real-time pricing, service optimization, and operational analytics.

Airbnb
	Platform: Uses Amazon S3 and Presto for querying.
	Use Case: Airbnb’s data lake stores data related to bookings and user interactions, 	helping improve the user experience through targeted recommendations and service 	improvements.

Capital One
	Platform: Leveraging AWS S3 and Databricks.
	Use Case: Capital One stores customer transaction data in a data lake to improve fraud 	detection, personalize customer interactions, and enhance product offerings.

4. Additional Examples of Data Lakes:

NASA’s Jet Propulsion Laboratory
	Platform: Utilizes Dell EMC Isilon and Hadoop for its data lake.
	Use Case: Stores and analyzes scientific research data, satellite imagery, and mission logs 	for various space exploration projects.

Zillow
	Platform: Uses Apache Spark on AWS.
	Use Case: Zillow’s data lake aggregates data from various sources, including real estate 	listings and user interactions, to provide insights for home buyers and sellers.
	

DATA Lake on AWS:
https://www.altexsoft.com/blog/data-lake-architecture/
Amazon Web Services (AWS) offers a robust data lake architecture anchored by its highly available and low-latency Amazon S3 storage service. S3 is particularly attractive for those looking to take advantage of AWS's expansive ecosystem, including complementary services like Amazon Aurora for relational databases.
	
Creating a data lake on AWS involves several steps, from setting up the storage to configuring data ingestion, transformation, and analytics services. Below is a detailed guide on how to create a data lake on AWS:

Step 1: Set Up Your AWS Environment
	Create an AWS Account:

	If you don’t have an AWS account, sign up at the AWS website.
	Set Up IAM Roles and Permissions:

	Create IAM roles with permissions that allow access to AWS services like S3, Glue, Lake 	Formation, and others. This ensures secure access to resources in your data lake.
Step 2: Create an Amazon S3 Bucket
	Open the S3 Console:

	Navigate to the Amazon S3 console.
	Create a New Bucket:

	Click on Create bucket.
	Provide a unique name and select the appropriate region.
	Configure options such as versioning, logging, and encryption based on your requirements.
	Set Permissions:

	Configure permissions to allow access for your data ingestion and processing services 	(e.g., Glue, Athena). Ensure that you set the right bucket policies for security.
Step 3: Use AWS Lake Formation (Optional but Recommended)
	Open AWS Lake Formation:

	Navigate to the Lake Formation console.
	Set Up Lake Formation:

	Register your S3 bucket with Lake Formation. This allows you to manage your data lake 	more efficiently, including permissions and access control.
	Enable data lake settings, including security and governance policies.
Step 4: Data Ingestion
	Using AWS Glue:

	Open AWS Glue Console: Navigate to the AWS Glue console.
	Create a Crawler:
	Crawlers help discover and catalog your data. Click on Crawlers and then Add crawler.
	Specify the data source (your S3 bucket) and schedule the crawler to run periodically.
	Create ETL Jobs:
	Use AWS Glue to create ETL jobs that transform data before loading it into your data lake. 	Glue can automatically generate code for ETL tasks based on your data schema.
	Using Amazon Kinesis for Streaming Data (if applicable):

	If you have streaming data, use Amazon Kinesis to capture and process real-time data 	streams.
	You can configure Kinesis Data Firehose to load streaming data directly into your S3 	bucket.
Step 5: Data Cataloging
	AWS Glue creates a data catalog of your ingested data, which allows you to understand 	the structure and format of the data stored in your data lake. This catalog can be accessed 	by other AWS services for querying and analytics.
Step 6: Querying the Data
	Using Amazon Athena:

	Open Amazon Athena Console: Navigate to the Athena console.
	Configure Athena: Set up the query location in S3 where Athena will store query results.
	Run Queries: Use SQL queries to analyze data directly from your S3 bucket without the 	need to move it to another database.
	Integrating with BI Tools:

	You can also use tools like Amazon QuickSight for data visualization and reporting, 	connecting it to your data lake for insights.
Step 7: Monitoring and Security
	Monitor Your Data Lake:
	Use AWS CloudTrail and AWS CloudWatch for monitoring access and usage of your data 	lake.
	Data Security:
	Implement data encryption at rest and in transit, and manage access through IAM roles 	and Lake Formation permissions.
	Example Use Case:
	Here’s a simplified use case for creating a data lake for an e-commerce company:

	Set up an S3 bucket named ecommerce-data-lake.
	Use AWS Glue to create a crawler that discovers sales transaction files uploaded to the 	bucket weekly.
	Create ETL jobs to transform raw data into structured formats suitable for analysis.
	Use Amazon Athena to query sales data for reporting and insights.
	Integrate Amazon QuickSight to visualize sales trends and customer behaviors.
Conclusion:
	